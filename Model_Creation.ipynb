{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f19314d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_7 (Dense)             (None, 21, 2)             6         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 42)                0         \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 21, 2)             0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 21, 2)             6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12\n",
      "Trainable params: 12\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\ENASSUK\\OneDrive - Ericsson\\Downloads\\Python_Code_For_GIThub\\HandGesture_Model\\assets\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Input\n",
    "from keras.layers.core import Dense,Flatten\n",
    "from keras.layers import InputLayer\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from  matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def create_dataset(IMG_WIDTH, IMG_HEIGHT, img_folder):\n",
    "   \n",
    "    img_data_array=[]\n",
    "    class_name=[]\n",
    "   \n",
    "    for dir1 in os.listdir(img_folder):\n",
    "        for file in os.listdir(os.path.join(img_folder, dir1)):\n",
    "       \n",
    "            image_path= os.path.join(img_folder, dir1,  file)\n",
    "            image= cv2.imread( image_path, cv2.COLOR_BGR2RGB)\n",
    "            image=cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH),interpolation = cv2.INTER_AREA)\n",
    "            image=np.array(image)\n",
    "#             image= np.resize(image,(IMG_HEIGHT,IMG_WIDTH,3))\n",
    "            image = image.astype('float32')\n",
    "            image /= 255 \n",
    "            img_data_array.append(image)\n",
    "            class_name.append(dir1)\n",
    "    return img_data_array, class_name# extract the image array and class name\n",
    "\n",
    "if __name__=='__main__':\n",
    "\n",
    "    IMG_WIDTH=21\n",
    "    IMG_HEIGHT=2\n",
    "    img_folder=r'C:\\Users\\ENASSUK\\OneDrive - Ericsson\\Downloads\\Python_Code_For_GIThub\\HandGesture_Dataset\\images'\n",
    "\n",
    "    img_data, class_name =create_dataset(IMG_WIDTH, IMG_HEIGHT, img_folder)\n",
    "    \n",
    "    target_dict={k: v for v, k in enumerate(np.unique(class_name))}\n",
    "    target_val=  [target_dict[class_name[i]] for i in range(len(class_name))]\n",
    "    \n",
    "    with open(r'C:\\Users\\ENASSUK\\OneDrive - Ericsson\\Downloads\\Python_Code_For_GIThub\\list_of_gestures.names','w+') as f:\n",
    "        for key,values in target_dict.items():\n",
    "            f.write(str(key) + \"\\n\")\n",
    "            \n",
    "    # Creating a simple deep learning model and compiling it\n",
    "\n",
    "    model=tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Dense(2, input_shape=[IMG_WIDTH,IMG_HEIGHT], activation='relu'),\n",
    "#             tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=(21, 2), activation='relu'),\n",
    "#             tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=(21, 2), activation='relu'),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Reshape([21, 2], input_shape=(42,)),\n",
    "            tf.keras.layers.Dense(2, activation='relu'),\n",
    "        ])\n",
    "    \n",
    "    model.summary()\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#     history = model.fit(x=np.array(img_data, np.float32), y=np.array(list(map(int,target_val)), np.float32), epochs=2)\n",
    "#     history = model.fit(x=tf.cast(np.array(img_data), tf.float64), y=tf.cast(list(map(int,target_val)),tf.int32), epochs=2)\n",
    "    model.save(r'C:\\Users\\ENASSUK\\OneDrive - Ericsson\\Downloads\\Python_Code_For_GIThub\\HandGesture_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9479af85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0e1279",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
